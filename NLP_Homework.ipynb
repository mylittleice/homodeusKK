{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lesson-01 Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 今天是2019年9月28日，今天世界上又多了一名AI工程师 :) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`各位同学大家好，欢迎各位开始学习我们的人工智能课程。这门课程假设大家不具备机器学习和人工智能的知识，但是希望大家具备初级的Python编程能力。根据往期同学的实际反馈，我们课程的完结之后 能力能够超过80%的计算机人工智能/深度学习方向的硕士生的能力。`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 本次作业的内容"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 复现课堂代码\n",
    "\n",
    "在本部分，你需要参照我们给大家的GitHub地址里边的课堂代码，结合课堂内容，复现内容。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 请回答以下问题\n",
    "\n",
    "回答以下问题，并将问题发送至 minchuian.gao@gmail.com 中：\n",
    "```\n",
    "    2.1. what do you want to acquire in this course？\n",
    "    2.2. what problems do you want to solve？\n",
    "    2.3. what’s the advantages you have to finish you goal?\n",
    "    2.4. what’s the disadvantages you need to overcome to finish you goal?\n",
    "    2.5. How will you plan to study in this course period?\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 如何提交\n",
    "代码 + 此 jupyter 相关，提交至自己的 github 中(**所以请务必把GitHub按照班主任要求录入在Trello中**)；\n",
    "第2问，请提交至minchuian.gao@gmail.com邮箱。\n",
    "#### 4. 作业截止时间\n",
    "此次作业截止时间为 2019.10.8日"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. 完成以下问答和编程练习"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 基础理论部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 0. Can you come up out 3 sceneraies which use AI methods? "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {淘宝推荐系统，谷歌翻译，自动驾驶，人脸识别}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. How do we use Github; Why do we use Jupyter and Pycharm;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans: {Github可以用来提交和分享代码，分享信息，协作项目\n",
    "      Jupyter在讲解代码和图像显示上有优势\n",
    "      Pycharm适合大型项目的编写}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. What's the Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:概率模型是利用词句出现的概率高低而不是具体的语法来判断句子的对错。\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Can you came up with some sceneraies at which we could use Probability Model?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "1、NLP在不同场合有不同的含义，在心理学上，NLP是神经语言编程学的缩写，简单的来说就是关于人类行为与沟通程序的一套详细可行的模式，常常被用来了解人类的经验和行为，在人工智能领域确是自然语言处理。当NLP出现在一篇报道中时，人工智能如何识别这次汇报的NLP是指神经语言编程学还是自然语言处理呢？\n",
    "我觉得可以用概率模型来判断，通过专业领域的论文语料库，将跟NLP一起出现的高频词统计出来，再将两种NLP重复的高频词去掉，就得到了各自的高频词。然后把两种NLP高频词在这篇文章中出现的概率各自相乘，取大概率的NLP(这种方法很不精准，可能也算概率模型的一种应用吧)\n",
    "2、人在说话时往往有自己的习惯用语或者缩写，如果以后的人工智能要做到像贾维斯一样，势必要更清楚这些习惯性的用词，可以运用概率模型来分析说话者的用词习惯（但如何让机器自己学习用户的习惯呢？一种方法可能是让用户给机器反馈，有点麻烦但很实用，像是在培养自己的语音助手，说不定有些可行性。第二种方法就是像现在的只能音箱一样，让用户预设，但这个更麻烦，不可能列出所有的可能情况，不是我理想中的解决方案）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. Why do we use probability and what's the difficult points for programming based on parsing and pattern match?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    我们使用概率是因为实现过程和实际效果比直接用语法简单，通过统计语料库的高频词可知，字词出现的频率中，第一大概是第二的两倍，第二大概是第三的两倍，并可以以此类推，长尾效应很明显，使用概率可以很快判断大部分语句的对错，就好像被引用次数的多少是一篇论文质量高低的判断方法一样。\n",
    "    基于解析和模式识别的程序设计过程的难点，我认为是如何找到这样一种模式，就好像爱因斯坦说过的，盯着问题无法找到解决的方法。我认为难点绝不在于编程本身，而是如何找到解决问题的方法。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 5. What's the Language Model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    定义一种语法规则，按照这种规则生成或者判断句子"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 6. Can you came up with some sceneraies at which we could use Language Model?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    1、可以利用编程语言基于自己设计的语法模型去创造一种自己的加密语言\n",
    "    2、学习语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 7. What's the 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    某句话中一个字或词在语料库中出现的概率积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 8. What's the disadvantages and advantages of 1-gram language model;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:\n",
    "    有利因素在于可清楚的观察各字词出现的频率\n",
    "    不利因素在于无法发现各字词间的关系，无法判断真实的对错"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 9. What't the 2-gram models;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:某句话中相邻两个字或词同时出现的概率积"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 编程实践部分"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. 设计你自己的句子生成器"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "如何生成句子是一个很经典的问题，从1940s开始，图灵提出机器智能的时候，就使用的是人类能不能流畅和计算机进行对话。和计算机对话的一个前提是，计算机能够生成语言。\n",
    "\n",
    "计算机如何能生成语言是一个经典但是又很复杂的问题。 我们课程上为大家介绍的是一种基于规则（Rule Based）的生成方法。该方法虽然提出的时间早，但是现在依然在很多地方能够大显身手。值得说明的是，现在很多很实用的算法，都是很久之前提出的，例如，二分查找提出与1940s, Dijstra算法提出于1960s 等等。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在著名的电视剧，电影《西部世界》中，这些机器人们语言生成的方法就是使用的SyntaxTree生成语言的方法。\n",
    "\n",
    "> \n",
    ">\n",
    "\n",
    "![WstWorld](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1569578233461&di=4adfa7597fb380e7cc0e67190bbd7605&imgtype=0&src=http%3A%2F%2Fs1.sinaimg.cn%2Flarge%2F006eYYfyzy76cmpG3Yb1f)\n",
    "\n",
    "> \n",
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "在这一部分，需要各位同学首先定义自己的语言。 大家可以先想一个应用场景，然后在这个场景下，定义语法。例如：\n",
    "\n",
    "在西部世界里，一个”人类“的语言可以定义为：\n",
    "``` \n",
    "human = \"\"\"\n",
    "human = 自己 寻找 活动\n",
    "自己 = 我 | 俺 | 我们 \n",
    "寻找 = 看看 | 找找 | 想找点\n",
    "活动 = 乐子 | 玩的\n",
    "\"\"\"\n",
    "```\n",
    "\n",
    "一个“接待员”的语言可以定义为\n",
    "```\n",
    "host = \"\"\"\n",
    "host = 寒暄 报数 询问 业务相关 结尾 \n",
    "报数 = 我是 数字 号 ,\n",
    "数字 = 单个数字 | 数字 单个数字 \n",
    "单个数字 = 1 | 2 | 3 | 4 | 5 | 6 | 7 | 8 | 9 \n",
    "寒暄 = 称谓 打招呼 | 打招呼\n",
    "称谓 = 人称 \n",
    "人称 = 先生 | 女士 | 小朋友\n",
    "打招呼 = 你好 | 您好 \n",
    "询问 = 请问你要 | 您需要\n",
    "业务相关 = 玩玩 具体业务\n",
    "玩玩 = 耍一耍 | 玩一玩\n",
    "具体业务 = 喝酒 | 打牌 | 打猎 | 赌博\n",
    "结尾 = 吗？\"\"\"\n",
    "\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请定义你自己的语法: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第一个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "buy_grammar = '''\n",
    "buy = hello please place do thing doubt\n",
    "hello = hi， | 你好！ | anybody?\n",
    "please = 请问 | 在 | \n",
    "place = 711 | 动物园 | 停车场 | 旅馆\n",
    "do = 做 | 卖 | 要 | 有\n",
    "thing = 包子 | 鸡 | 水 | 玩具\n",
    "doubt = 吗？ | 吧！\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "第二个语法："
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "help_grammar = '''\n",
    "help = hello need something doubt\n",
    "hello = hi, | 你好， | 请问 | 嘿！\n",
    "need = 需要 | 想要 \n",
    "something = 包子 | 鸡 | 水 | 玩具\n",
    "doubt = 吗？ | 吧！\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi，711卖包子吗？\n",
      "hi,需要玩具吧！\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "def generate_by_grammar(words_grammar,target,srmt_split='=',prot_split='|'):\n",
    "    gram_dict = {}\n",
    "    for line in words_grammar.split('\\n'):\n",
    "        if not line:continue\n",
    "        srmt,prot = line.strip().split(srmt_split)\n",
    "        gram_dict[srmt.strip()] = prot.strip().split(prot_split)\n",
    "    return generate(gram_dict,target)\n",
    "\n",
    "def generate(gram_dict,target):\n",
    "    if target in gram_dict:\n",
    "        words = gram_dict[target]\n",
    "        word = random.choice(words).strip().split(' ')\n",
    "        return \"\".join(generate(gram_dict,target=i) for i in word)\n",
    "    else:\n",
    "        return target\n",
    "\n",
    "print(generate_by_grammar(buy_grammar,'buy'))\n",
    "print(generate_by_grammar(help_grammar,'help'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，使用自己之前定义的generate函数，使用此函数生成句子。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TODO: 然后，定义一个函数，generate_n，将generate扩展，使其能够生成n个句子:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hi，请问停车场做水吧！\n",
      "你好，需要水吗？\n",
      "hi，动物园要水吧！\n",
      "你好，需要水吗？\n",
      "hi，在711做玩具吧！\n",
      "嘿！想要水吗？\n",
      "你好！请问711卖水吧！\n",
      "你好，想要包子吗？\n",
      "你好！在711做鸡吗？\n",
      "嘿！想要鸡吧！\n",
      "anybody?请问711要玩具吗？\n",
      "嘿！想要玩具吧！\n",
      "hi，请问旅馆卖玩具吧！\n",
      "hi,想要鸡吗？\n",
      "anybody?请问711要玩具吗？\n",
      "嘿！需要玩具吧！\n",
      "anybody?请问711有包子吗？\n",
      "嘿！需要鸡吧！\n",
      "hi，停车场要水吧！\n",
      "嘿！需要玩具吗？\n"
     ]
    }
   ],
   "source": [
    "def generate_n(word1_grammar,word2_grammar,target1,target2):\n",
    "    # you code here \n",
    "    for i in range(10):\n",
    "        print(generate_by_grammar(word1_grammar,target1))\n",
    "        print(generate_by_grammar(word2_grammar,target2))\n",
    "generate_n(buy_grammar,help_grammar,'buy','help')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. 使用新数据源完成语言模型的训练"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "按照我们上文中定义的`prob_2`函数，我们更换一个文本数据源，获得新的Language Model:\n",
    "\n",
    "1. 下载文本数据集（你可以在以下数据集中任选一个，也可以两个都使用）\n",
    "    + 可选数据集1，保险行业问询对话集： https://github.com/Computing-Intelligence/insuranceqa-corpus-zh/raw/release/corpus/pool/train.txt.gz\n",
    "    + 可选数据集2：豆瓣评论数据集：https://github.com/Computing-Intelligence/datasource/raw/master/movie_comments.csv\n",
    "2. 修改代码，获得新的**2-gram**语言模型\n",
    "    + 进行文本清洗，获得所有的纯文本\n",
    "    + 将这些文本进行切词\n",
    "    + 送入之前定义的语言模型中，判断文本的合理程度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "376445\n"
     ]
    }
   ],
   "source": [
    "#清洗数据\n",
    "import re\n",
    "\n",
    "def deleten(pattern,moiveComment,firstNum):\n",
    "    num = firstNum\n",
    "    if not pattern.match(moiveComment[num+1]):\n",
    "        c = moiveComment[num]+ moiveComment[num+1]\n",
    "        moiveComment[num] = c\n",
    "        del moiveComment[num+1]\n",
    "        # print(c)\n",
    "        return deleten(pattern,moiveComment,num)\n",
    "    else:\n",
    "        return moiveComment\n",
    "with open(r'C:\\Users\\P51\\Desktop\\moive.txt') as f:\n",
    "#     print(f.read())\n",
    "#     print(len(str(f.read())))\n",
    "    moiveComment = str(f.read()).split('\\n')[1:]\n",
    "    words = ''\n",
    "    pattern = re.compile('\\d+,.*?')\n",
    "    try:\n",
    "        for i in range(len(moiveComment)-1):\n",
    "            if not moiveComment[i]:continue\n",
    "            moiveComment = deleten(pattern,moiveComment,i)\n",
    "    except IndexError:\n",
    "        pass\n",
    "    finally:\n",
    "        with open(r'C:\\Users\\P51\\Desktop\\moive01.txt','a') as fn:\n",
    "            for line in moiveComment:\n",
    "                fn.writelines(line+'\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Loading model from cache C:\\Users\\P51\\AppData\\Local\\Temp\\jieba.cache\n",
      "Loading model cost 1.438 seconds.\n",
      "Prefix dict has been built succesfully.\n"
     ]
    }
   ],
   "source": [
    "#切词\n",
    "file  = open(r'C:\\Users\\P51\\Desktop\\moive01.txt').read()\n",
    "files = ''\n",
    "for line in file.split('\\n'):\n",
    "    if not line:continue\n",
    "    *something,words,num = line.split(',')\n",
    "    files += words \n",
    "from collections import Counter\n",
    "#单词\n",
    "word_count = Counter(files)\n",
    "\n",
    "import jieba\n",
    "# files.most_common(10)\n",
    "tokens = list(jieba.cut(files))\n",
    "#双词\n",
    "two_gram = [tokens[i]+tokens[i+1] for i in range(len(tokens)-1)]\n",
    "# print(two_gram[:10])\n",
    "\n",
    "two_gram_count = Counter(two_gram)\n",
    "# two_gram_count.most_common()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_gram_count(word,thesaurus):\n",
    "    if word in thesaurus:return thesaurus[word]\n",
    "    else:\n",
    "        return thesaurus.most_common()[-1][-1]\n",
    "    \n",
    "def two_gram_model(sentence):\n",
    "    tokens = list(jieba.cut(sentence))\n",
    "    probability = 1\n",
    "    \n",
    "    for i in range(len(tokens)-1):\n",
    "        first_word = tokens[i]\n",
    "        second_word = tokens[i+1]\n",
    "\n",
    "        two_words_count = get_gram_count(first_word+second_word,two_gram_count)\n",
    "        next_words_count = get_gram_count(second_word,word_count)\n",
    "        \n",
    "        probability *= (two_words_count / next_words_count)\n",
    "    return probability"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "是\n",
      "杨\n",
      "3\n",
      "93\n",
      "杨\n",
      "幂\n",
      "25\n",
      "71\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.011358473421172193"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "two_gram_model('是杨幂')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. 获得最优质的的语言"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "当我们能够生成随机的语言并且能判断之后，我们就可以生成更加合理的语言了。请定义 generate_best 函数，该函数输入一个语法 + 语言模型，能够生成**n**个句子，并能选择一个最合理的句子: \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "提示，要实现这个函数，你需要Python的sorted函数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[1, 2, 3, 5]"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([1, 3, 5, 2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "这个函数接受一个参数key，这个参数接受一个函数作为输入，例如"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(1, 4), (2, 5), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第0个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(5, 0), (1, 4), (4, 4), (2, 5)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(2, 5), (1, 4), (4, 4), (5, 0)]"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sorted([(2, 5), (1, 4), (5, 0), (4, 4)], key=lambda x: x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "能够让list按照第1个元素进行排序, 但是是递减的顺序。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'anybody?请问旅馆卖水吗？'"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "buy_grammar = '''\n",
    "buy = hello please place do thing doubt\n",
    "hello = hi， | 你好！ | anybody?\n",
    "please = 请问 | 在 | \n",
    "place = 711 | 动物园 | 停车场 | 旅馆\n",
    "do = 做 | 卖 | 要 | 有\n",
    "thing = 包子 | 鸡 | 水 | 玩具\n",
    "doubt = 吗？ | 吧！\n",
    "'''\n",
    "\n",
    "def generate_best(sen_grammar,sen_model): # you code here\n",
    "    words_grammar = sen_grammar + sen_model\n",
    "    target,anything = sen_grammar.split('=')\n",
    "    senList = []\n",
    "    for i in range(10):\n",
    "        temp = generate_by_grammar(words_grammar,target.strip())\n",
    "        tempPro = two_gram_model(temp)\n",
    "        senList.append((tempPro,temp))\n",
    "    return sorted(senList,key=lambda x:x[0],reverse=True)[0][1]\n",
    "\n",
    "sen_grammar = 'buy = hello please place do thing doubt'\n",
    "sen_model = \"\"\"\n",
    "hello = hi， | 你好！ | anybody?\n",
    "please = 请问 | 在 | \n",
    "place = 711 | 动物园 | 停车场 | 旅馆\n",
    "do = 做 | 卖 | 要 | 有\n",
    "thing = 包子 | 鸡 | 水 | 玩具\n",
    "doubt = 吗？ | 吧！\n",
    "\"\"\"\n",
    "# target,anthing = sen_grammar.split('=')\n",
    "generate_best(sen_grammar,sen_model)\n",
    "# print(target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "好了，现在我们实现了自己的第一个AI模型，这个模型能够生成比较接近于人类的语言。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: 这个模型有什么问题？ 你准备如何提升？ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ans:这个模型比较依赖于它的两个输入，语法和语法模型，它生成的句子只能在这个范围内取概率最高的，应答能力很有限。而且只是生成一句概率较高的句子，用来问答是不行的，难免出现答非所问的情况。我理想中的要解决的问题是如何提取提问者话中的关键词，至少能回答与之相关的话，至少是含有相关词。最好的办法是用问来代替回答，避免暴露模型的缺点，让提问者变成回答者"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 以下内容为可选部分，对于绝大多数同学，能完成以上的项目已经很优秀了，下边的内容如果你还有精力可以试试，但不是必须的。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4. (Optional) 完成基于Pattern Match的语句问答\n",
    "> 我们的GitHub仓库中，有一个assignment-01-optional-pattern-match，这个难度较大，感兴趣的同学可以挑战一下。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "#### 5. (Optional) 完成阿兰图灵机器智能原始论文的阅读\n",
    "1. 请阅读阿兰图灵关于机器智能的原始论文：https://github.com/Computing-Intelligence/References/blob/master/AI%20%26%20Machine%20Learning/Computer%20Machinery%20and%20Intelligence.pdf \n",
    "2. 并按照GitHub仓库中的论文阅读模板，填写完毕后发送给我: mqgao@kaikeba.com 谢谢"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "各位同学，我们已经完成了自己的第一个AI模型，大家对人工智能可能已经有了一些感觉，人工智能的核心就是，我们如何设计一个模型、程序，在外部的输入变化的时候，我们的程序不变，依然能够解决问题。人工智能是一个很大的领域，目前大家所熟知的深度学习只是其中一小部分，之后也肯定会有更多的方法提出来，但是大家知道人工智能的目标，就知道了之后进步的方向。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "然后，希望大家对AI不要有恐惧感，这个并不难，大家加油！"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](https://timgsa.baidu.com/timg?image&quality=80&size=b9999_10000&sec=1561828422005&di=48d19c16afb6acc9180183a6116088ac&imgtype=0&src=http%3A%2F%2Fb-ssl.duitang.com%2Fuploads%2Fitem%2F201807%2F28%2F20180728150843_BECNF.thumb.224_0.jpeg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
